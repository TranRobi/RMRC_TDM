<!DOCTYPE html>
<html lang="en">
	<head>
		<meta charset="UTF-8" />
		<meta name="viewport" content="width=device-width, initial-scale=1.0" />
		<link rel="shortcut icon" href="../img/kc-logov2.ico" type="image/x-icon" />
		<link
			rel="stylesheet"
			href="https://fonts.googleapis.com/css2?family=Material+Symbols+Outlined:opsz,wght,FILL,GRAD@24,400,0,0"
            />
		<link rel="stylesheet" href="../css/style.css" />
		<title>Kókány Corporations</title>
	</head>
	<body>
		<nav>
			<div class="logo">
				<div class="img">
					<img src="../img/kc-logov2.png" alt="logo" class="log" />
				</div>
				<h1>Kókány Corporations</h1>
			</div>
			<ul>
				<li>
					<a href="../index.html" title="Team"
						><span class="material-symbols-outlined">Groups</span></a
					>
				</li>
				<li>
					<a href="software.html" class="active" title="Software"
						><span class="material-symbols-outlined"> terminal </span></a
					>
				</li>
				<li>
					<a href="hardware.html" title="Hardware">
						<span class="material-symbols-outlined"> manufacturing </span>
					</a>
				</li>
				<li>
					<a href="attempts.html" title="Download"
						><span class="material-symbols-outlined"> download </span></a
					>
				</li>
			</ul>
		</nav>
		<div class="member-border">
			<div class="content">
				<h1>Our Software</h1>
                <p>
                    This page details some aspects of Kókánybot's software stack,
                    and goes into greater detail regarding some changes made since
                    <a href="http://2023.robocup.org/en/home/">the previous year</a>.
                </p>
				<h2>Communication</h2>
				<p>
					We interact with our robot using a few custom programs, named
					KókányControl (kokanyctl) and KókányRecognize (kokanyrecognize).
				</p>
				<p>
					KókányControl has a graphical interface for displaying the video and
					the sensor data it receives from Kókánybot. It takes keyboard input,
					and sends commands to the Raspberry Pi. It also recognizes QR codes
					that appear on Kókánybot’s cameras.
				</p>
				<p>
					KókányRecognize was written to reduce the complexity of KókányControl,
					since image recognition functionality is only needed in a few runs,
					and we can just launch KókányRecognize whenever we need it. This also
					enabled us to build KókányControl in pure C, since we would have
					needed to use C++ to build the image recognition bits (which uses
					OpenCV).
				</p>
				<p>The robot can be controlled via WiFi (2.4Ghz) or Ethernet.</p>

				<h2>Training an object detection model</h2>

				<p>
					Last time, one of the most challenging aspects of the competition was
					the object detection feature robots needed. Back in Bordeaux, we
					failed to detect anything and got 0 points for object detection. We
					definitely needed to improve on that.
				</p>

				<p>
					Firstly, we found a large enough dataset on the internet (1k+ photos),
					which was a godsend because it spared us from having to manually take
					pictures and tag them.
				</p>

				<p>
					Secondly, we switched from the outdated YoloV5 to the up-to-date
					YoloV8 which is supposedly better in every single way. We chose the
					small version, because the larger the model, the slower inference is,
					and while performance doesn’t really matter when training, one can
					simply leave their computer running while they are not home, it is
					still vital during inference, because our laptops are not on par with
					our workstations at home.
				</p>

				<p>
					To use our model, we wrote a script named kokanyrecognize at the last
					minute. It wasn’t very performant nor really clean, so we spent a
					significant amount of time working on it. So far it was rewritten to
					use the new model, however it still has a long way to go.
				</p>
				<p>Here is the result so far:</p>
				<img
					src="../img/inference.jpg"
					alt="Image showcasing the accuracy of our model"
				/>
				<p>
					To implement the actual capturing and transmission we used a simple
					FFmpeg commandline in a
					<a
						href="https://github.com/zsoltiv/kokanybot/blob/master/kokanystream.sh"
						target="_blank"
						>shell script</a
					>. We picked FFmpeg because as its
					<a href="https://ffmpeg.org" target="_blank">project page</a>
					suggests, it's a multimedia swiss army knife that can handle a wide
					array of use cases, including
					<a
						href="http://trac.ffmpeg.org/wiki/StreamingGuide#Pointtopointstreaming"
						target="_blank"
						>point-to-point</a
					>
					streaming via UDP, allowing us to reduce the infrastructure required
					to transmit the audiovisual data.
				</p>
				<p>
					On the client side, we implemented the decoding of the video data
					using FFmpeg’s libavformat and libavcodec libraries. Rendering the
					video frames was tricky to figure out because pretty much all video
					encoders store pixels in YCbCr colour space, which SDL isn’t the best
					for.
				</p>
				<p>
					The APIs of the libav* libraries are <b>huge</b>. Thankfully we only
					really needed the
					<a
						href="https://ffmpeg.org/doxygen/trunk/group__lavc__encdec.html"
						target="_blank"
						>high level decoding API</a
					>. The
					<a
						href="https://github.com/namndev/FFmpegTutorial/blob/master/learn-ffmpeg-libav-the-hard-way.md"
						target="_blank"
						>Learn FFmpeg libav the Hard Way</a
					>
					tutorial combined with the
					<a
						href="https://git.ffmpeg.org/gitweb/ffmpeg.git/tree/HEAD:/doc/examples"
						target="_blank"
						>examples</a
					>
					in the project's documentation also made things much easier.
				</p>
				<h2>Simplifying our software with device trees</h2>
				<p>
					Last year we used a library called
					<a
						href="https://git.kernel.org/pub/scm/utils/i2c-tools/i2c-tools.git/about/"
					>
						libi2c</a
					>
					to control some of our stepper motors using an external
					<a href="https://www.microchip.com/en-us/product/MCP23017"
						>MCP23017 GPIO expander</a
					>. Since then we have switched to using servos for our robotic arm.
                    We also wanted to make our software more flexible, and the best way to
					do this was ripping out the useless code and using the right
					tools for the job.
				</p>
				<p>
					While programs are allowed to just use libi2c, it adds (often
					redundant) extra code to a project. The Kernel actually has drivers
					for a lot of common ICs such as GPIO expanders—like the MCP23017 we
                    used—or PWM controllers like the <a href="https://www.adafruit.com/product/815">PCA9685</a>.
				</p>
				<p>
					One improvement we've made was deleting the I2C parts from our code,
					and instead using the
					<a
						href="https://elixir.bootlin.com/linux/latest/source/drivers/pwm/pwm-pca9685.c"
						>kernel driver</a
					>
					instead. This is great because we can just interact with the userspace PWM API via something
                    like <a href="https://github.com/zsoltiv/libhwpwm/tree/master">libhwpwm</a> (more on that later).
				</p>
				<p>
					To achieve this, we wrote a
					<a
						href="https://github.com/zsoltiv/kokanybot/blob/master/overlays/kokanyservoctl.dts"
						>device tree overlay</a
					>. Device tree overlays are kind of like patch files for
					<a
						href="https://www.kernel.org/doc/html/latest/devicetree/usage-model.html"
						>device trees</a
					>. This file allows us to tell Linux what chips are available on certain
					I2C addresses.
				</p>
                <h2>Switching from TCP to UDP</h2>
                <p>
                    Robotics taught us just how fragile computers are when subjected to harsher environments.
                    Short circuits may occur, components might get knocked against a robot's frame, boards can overheat,
                    all of which can result in a robot rebooting or shutting down entirely.
                </p>
                <p>
                    In our previous competitions, we have <i>always</i> had faults like this occur. If things were going too well,
                    then our UTP cable slipped out of our control station's Ethernet port. In these cases, we almost always had to
                    reboot our robot and do the software side of the setup again, wasting precious time.
                </p>
                <p>
                    In an attempt to mitigate these issues issues while sticking with TCP sockets would mean having
                    to handle dozens of possible states, many of which might only occur during the actual competition.
                    Instead of doing that, we've been making steady progress to switch over to UDP,
                    which is a different communications protocol. As of now, the code which receives keyboard
                    input uses no TCP sockets, it's all stateless.
                </p>
                <p>
                    The main advantage of UDP compared to TCP is its statelessness: networked programs do not need to
                    <code>connect()</code> or <code>listen()</code> nor <code>accept()</code>; you create a socket, and
                    send/receive data over it using <code>sendto()</code> and <code>recvfrom()</code>.
                    This eliminates the need for handling reconnections.
                </p>
                <p>
                    The main downsides of UDP are that it does not guarantee data packets arriving in the correct order
                    nor does it guarantee that they arrive <i>at all</i>. This is not really an issue for us,
                    since the entire network is point-to-point with no more than two hosts, and we have yet to experience
                    any problems.
                </p>
                <h2>Removing gas sensing code</h2>
                <p>
                    The rulebook's <a href="https://oarkit.intelligentrobots.org/home/wp-content/uploads/2023/10/RoboCupRescue-RMRC-2024-Rulebook-working-draft-2023-10-15.pdf">latest draft</a> does not mention CO<sub>2</sub> sensing, therefore we've removed all code related to it. If a later draft brings it back,
                    we can just reuse a previous commit, as we've often done during development.
                </p>
				<h2>Video and audio streaming</h2>
				<p>
					During tests, operators are only allowed to see the arenas from their
					robot’s point of view. This meant we needed a way to find a way to
					display the video data from the robot’s cameras.
				</p>
                <p>
                    We've learnt from our mistakes last year, and have opted for using multiple cameras so
                    that we have better peripheral vision while controlling Kókánybot.
                </p>
				<p>
					Multimedia related tasks are surprisingly computation heavy when one
					is working with embedded systems. The CPU in the Raspberry Pi 4B+ is
					fairly capable, however we also had to consider power draw and thermal
					related problems. We considered several video formats: H.265, AV1 and
					H.264, but in the end we settled on using raw frames from our cameras
                    to minimize latency as much as possible, since at the 2023 RoboCup,
                    our camera's high latency has caused a lot of trouble.
				</p>
                <p>
                    One of our cameras is currently a
                    <a href="https://www.raspberrypi.com/documentation/accessories/camera.html">Raspberry Pi Official Camera Module</a>.
                    This uses the new and improved libcamera stack which does not play well with regular
                    V4L2 programs—such as FFmpeg—hence we wrote a
                    <a href="https://github.com/zsoltiv/kokanybot/blob/master/kokanystream-front.sh">modified script</a> to work with the Pi Camera.
                </p>
                <h2>libhwpwm</h2>
                <p>
                    As we've stated above, one of our main goals was to reduce the number of barely working
                    DIY solutions, and rely on what's tried and tested: The kernel drivers. To that end, we
                    did not want to bring back our
                    <a href="https://github.com/zsoltiv/kokanybot/blob/mirk2023/src/pca9685.c">old PCA9685 driver</a>.
                </p>
                <p>
                    There was an issue however: There was no thin, low-level C library for poking
                    the sysfs PWM files safely (or at least as safely as C code can). As it's written in
                    the <a href="https://zorleone.xyz/posts/announcing-libhwpwm/">libhwpwm announcement</a>,
                    the <a href="https://github.com/zeroping/sysfspwm">existing solutions</a> were too complex for our taste,
                    so contrary to what we set out to do, <a href="https://github.com/zsoltiv/libhwpwm">we rolled our own solution</a>,
                    but the result is less hacky than the aforementioned old PWM code.
                </p>
                <p>
                    The implementation is really simple as the library only needs to poke ~8 or so files in total.
                    The library has been
                    <a href="https://github.com/zsoltiv/libhwpwm/commit/6d2fea4250a91d9d32f8358b7282dce7be7fb308">rewritten entirely</a>
                    when we uncovered the troubles that holding onto critical file descriptors—sysfs files—cause.
                </p>
                <p>
                    We've also encountered a rather <a href="https://forums.raspberrypi.com/viewtopic.php?t=287015">exotic bug in the
                    Raspberry Pi and the camera modules</a>, so we will reconsider using those modules.
                </p>
				<h2>Setup and packup of the robot and operator station</h2>
				<p>
					To set up the robot, we slide a battery into our battery holder,
					and flip a switch on our robot.
				</p>
				<p>
					For the operating station, we use a laptop. We connect the laptop to the
                    robot using an Ethernet cable, and run <code>kokanyctl 192.168.69.1</code>.
				</p>
				<h2>Mission strategy</h2>
				<p>
					We use standard, easily acquirable parts, which makes our robot truly
					rapidly manufacturable.
				</p>
				<h2>Experiments and Testing</h2>
				<p>
					We have conducted image recognition tests, and tested the movement of
					our robot in our school. Our image recognition model successfully
					recognized hazmat labels with an 80% accuracy. We have also tested
                    our robotic arm controls, they worked good enough to gain us a decent amount
                    of points at the 2024 Hungarian competition (see the main page).
				</p>
				<h2>Software packages used</h2>
				<table>
					<tbody>
						<tr class="odd">
							<td>
								<a
									href="https://git.kernel.org/pub/scm/libs/libgpiod/libgpiod.git/about/"
									target="_blank"
								>
									libgpiod
								</a>
							</td>
							<td>DC motor control, sensor control, stepper control</td>
						</tr>
						<tr class="even">
							<td>
								<a href="https://opencv.org/" target="_blank"> OpenCV </a>
							</td>
							<td>Image recognition</td>
						</tr>
						<tr class="odd">
							<td>
								<a href="https://ffmpeg.org/" target="_blank"> FFmpeg </a>
							</td>
							<td>
								Video streaming, Image recognition backend (used by OpenCV)
							</td>
						</tr>
						<tr class="even">
							<td>
								<a href="https://www.libsdl.org/" target="_blank"> SDL2 </a>
							</td>
							<td>
								Used by KókányControl to process keyboard input and display
								video
							</td>
						</tr>
						<tr class="odd">
							<td>
								<a href="https://github.com/libsdl-org/SDL_ttf" target="_blank">
									SDL2_ttf
								</a>
							</td>
							<td>
								Used by KókányControl to draw text for displaying sensor data
							</td>
						</tr>
						<tr class="even">
							<td>
								<a href="https://github.com/libsdl-org/SDL_net" target="_blank">
									SDL2_net
								</a>
							</td>
							<td>Used by KókányControl to handle networking</td>
						</tr>
						<tr class="odd">
							<td>
								<a href="https://zbar.sourceforge.net/" target="_blank">
									libzbar
								</a>
							</td>
							<td>Used for QR code detection</td>
						</tr>
						<tr class="even">
							<td>
								<a
									href="https://github.com/ultralytics/ultralytics"
									target="_blank"
								>
									YoloV8 (small)
								</a>
							</td>
							<td>The model we use for object detection</td>
						</tr>
						<tr class="even">
							<td>
								<a
									href="https://universe.roboflow.com/new-workspace-xqnz7/rmrc-dqa9p"
									target="_blank"
								>
									RMRC Dataset
								</a>
							</td>
							<td>Dataset used for training our image recognition model</td>
						</tr>
						<tr class="odd">
							<td>
								<a href="https://undefined-medium.com/" target="_blank">
									undefined medium
								</a>
							</td>
							<td>The font used in kokanyctl</td>
						</tr>
						<tr class="even">
							<td>
								<a
									href="https://github.com/zsoltiv/libhwpwm/tree/master"
									target="_blank"
								>
									libhwpwm
								</a>
							</td>
                            <td>
                                A C library to interface with the
                                <a href="https://www.kernel.org/doc/html/latest/driver-api/pwm.html#using-pwms-with-the-sysfs-interface">
                                    Linux PWM userspace API
                                </a>
                            </td>
						</tr>
						<tr class="odd">
							<td>
								<a href="https://github.com/raspberrypi/rpicam-apps" target="_blank">
									rpicam-apps
								</a>
							</td>
							<td>The programs used to capture the Pi Camera's video data</td>
						</tr>
					</tbody>
				</table>
				<h2>What the team has learned so far</h2>
				<ul>
					<li>
						We gained experience with Fusion360, TinkerCAD and 3D printing
					</li>
					<li>
						<p>
							Using C enabled us to learn a lot about interacting with hardware
							at a lower level:
						</p>

						<ul>
							<li>Using standard Linux APIs</li>
							<li>Using I2C to communicate with integrated circuits</li>
							<li>Reading datasheets</li>
							<li>
								Learned to embed FFmpeg in C programs using their various
								<a
									href="https://ffmpeg.org/doxygen/trunk/index.html"
									target="_blank"
									>libraries</a>
							</li>
                            <li>Learned a lot about the quirks of PWM drivers in Linux, and their userspace API</li>
						</ul>
					</li>
				</ul>
			</div>
		</div>
	</body>
</html>
