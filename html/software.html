<!DOCTYPE html>
<html lang="en">
	<head>
		<meta charset="UTF-8" />
		<meta name="viewport" content="width=device-width, initial-scale=1.0" />
		<link rel="shortcut icon" href="../img/kc-logov2.ico" type="image/x-icon" />
		<link
			rel="stylesheet"
			href="https://fonts.googleapis.com/css2?family=Material+Symbols+Outlined:opsz,wght,FILL,GRAD@24,400,0,0"
		/>
		<link rel="stylesheet" href="../css/style.css" />
		<title>Kókány Corporations</title>
	</head>
	<body>
		<nav>
			<div class="logo">
				<div class="img">
					<img src="../img/kc-logov2.png" alt="logo" class="log" />
				</div>
				<h1>Kókány Corporations</h1>
			</div>
			<ul>
				<li>
					<a href="../index.html" title="Team"
						><span class="material-symbols-outlined">Groups</span></a
					>
				</li>
				<li>
					<a href="software.html" class="active" title="Software"
						><span class="material-symbols-outlined"> terminal </span></a
					>
				</li>
				<li>
					<a href="hardware.html" title="Hardware">
						<span class="material-symbols-outlined"> manufacturing </span>
					</a>
				</li>
				<li>
					<a href="attempts.html" title="Attempts"
						><span class="material-symbols-outlined"> download </span></a
					>
				</li>
			</ul>
		</nav>
		<div class="member-border">
			<div class="content">
				<h1>Our Software</h1>
				<h2>Communication</h2>
				<p>
					We interact with our robot using a few custom programs, named
					KókányControl (kokanyctl) and KókányRecognize (kokanyrecognize).
				</p>
				<p>
					KókányControl has a graphical interface for displaying the video and
					the sensor data it receives from Kókánybot. It takes keyboard input,
					and sends commands to the Raspberry Pi. It also recognizes QR codes
					that appear on Kókánybot’s camera.
				</p>
				<p>
					KókányRecognize was written to reduce the complexity of KókányControl,
					since image recognition functionality is only needed in a few runs,
					and we can just launch KókányRecognize whenever we need it. This also
					enabled us to build KókányControl in pure C, since we would have
					needed to use C++ to build the image recognition bits (which usestl"
					OpenCV).
				</p>
				<p>The robot can be controlled via WiFi (2.4Ghz) or Ethernet.</p>

				<h2>Training an object detection model</h2>

				<p>
					Last time, one of the most challanging aspects of the competition was
					the object detection feature robots needed. Back in in Bordeaux, we
					failed to detect anything and got 0 points for object detection. We
					definitely needed to improve on that.
				</p>

				<p>
					Firstly, we found a large enough dataset on the internet (1k+ photos),
					which was a godsend because it spared us from having to manually take
					pictures and tag them.
				</p>

				<p>
					Secondly, we switched from the outdated YoloV5 to the up-to-date
					YoloV8 which is supposedly better in every single way. We chose the
					small version, because the larger the model, the slower inference is,
					and while performance doesn’t really matter when training, one can
					simply leave their computer running while they are not home, it is
					still vital during inference, because our laptops are not on par with
					our workstations at home.
				</p>

				<p>
					To use our model, we wrote a script named kokanyrecognize at the last
					minute. It wasn’t very performant nor really clean, so we spent a
					significant amount of time working on it. So far it was rewritten to
					use the new model, however it still has a long way to go.
				</p>
				<p>Here is the result so far:</p>
				<img
					src="../img/inference.jpg"
					alt="Image showcasing the accuracy of our model"
				/>
				<p>
					To implement the actual capturing and transmission we used a simple
					FFmpeg commandline in a
					<a
						href="https://github.com/zsoltiv/kokanybot/blob/master/kokanystream.sh"
						target="_blank"
						>shell script</a
					>. We picked FFmpeg because as its
					<a href="https://ffmpeg.org" target="_blank">project page</a>
					suggests, it's a multimedia swiss army knife that can handle a wide
					array of use cases, including
					<a
						href="http://trac.ffmpeg.org/wiki/StreamingGuide#Pointtopointstreaming"
						target="_blank"
						>point-to-point</a
					>
					streaming via UDP, allowing us to reduce the infrastructure required
					to transmit the audiovisual data.
				</p>
				<p>
					On the client side, we implemented the decoding of the video data
					using FFmpeg’s libavformat and libavcodec libraries. Rendering the
					video frames was tricky to figure out because pretty much all video
					encoders store pixels in YCbCr colour space, which SDL isn’t the best
					for.
				</p>
				<p>
					The APIs of the libav* libraries are <b>huge</b>. Thankfully we only
					really needed the
					<a
						href="https://ffmpeg.org/doxygen/trunk/group__lavc__encdec.html"
						target="_blank"
						>high level decoding API</a
					>. The
					<a
						href="https://github.com/namndev/FFmpegTutorial/blob/master/learn-ffmpeg-libav-the-hard-way.md"
						target="_blank"
						>Learn FFmpeg libav the Hard Way</a
					>
					tutorial combined with the
					<a
						href="https://git.ffmpeg.org/gitweb/ffmpeg.git/tree/HEAD:/doc/examples"
						target="_blank"
						>examples</a
					>
					in the project's documentation also made things much easier.
				</p>
				<h2>Simplifying our software with device trees</h2>
				<p>
					last year we used a library called
					<a
						href="https://git.kernel.org/pub/scm/utils/i2c-tools/i2c-tools.git/about/"
					>
						libi2c</a
					>
					to control some of our stepper motors using an external
					<a href="https://www.microchip.com/en-us/product/MCP23017"
						>MCP23017 GPIO expander</a
					>. Since then we have switched to using servos for our robotic arm.
                    We also wanted to make our software more flexible, and the best way to
					do this was ripping out the useless code and using the right
					tools for the job.
				</p>
				<p>
					While programs are allowed to just use libi2c, it adds (often
					redundant) extra code to a project. The Kernel actually has drivers
					for a lot of common ICs such as GPIO expanders—like the MCP23017 we
                    used—or PWM controllers like the <a href="https://www.adafruit.com/product/815">PCA9685</a>.
				</p>
				<p>
					One improvement we've made was deleting the I2C parts from our code,
					and instead using the
					<a
						href="https://elixir.bootlin.com/linux/latest/source/drivers/pwm/pwm-pca9685.c"
						>kernel driver</a
					>
					instead. This is great because we can just interact with the userspace PWM API via something
                    like <a href="https://github.com/zsoltiv/libhwpwm/tree/master">libhwpwm</a> (more on that later).
				</p>
				<p>
					To achieve this, we wrote a
					<a
						href="https://github.com/zsoltiv/kokanybot/blob/master/kokanyservoctl.dts"
						>device tree overlay</a
					>. Device tree overlays are kind of like patch files for
					<a
						href="https://www.kernel.org/doc/html/latest/devicetree/usage-model.html"
						>device trees</a
					>. This file allows us to tell Linux what chips are available on certain
					I2C addresses.
				</p>
				<h2>Setup and packup of the robot and operator station</h2>
				<p>
					To set up the robot, we slide a battery into our battery holder,
					and flip a switch on our robot.
				</p>
				<p>
					For the operating station, we use a laptop. We connect the laptop to the
                    robot using an Ethernet cable, and run <code>kokanyctl 192.168.69.1</code>.
				</p>
				<h2>Mission strategy</h2>
				<p>
					We use standard, easily acquirable parts, which makes our robot truly
					rapidly manufacturable.
				</p>
				<h2>Video and audio streaming</h2>
				<p>
					During tests, operators are only allowed to see the arenas from their
					robot’s point of view. This meant we needed a way to find a way to
					display the video data from the robot’s camera.
				</p>
				<p>
					Multimedia related tasks are surprisingly computation heavy when one
					is working with embedded systems. The CPU in the Raspberry Pi 4B+ is
					fairly capable, however we also had to consider power draw and thermal
					related problems. We considered several video formats: H.265, AV1 and
					H.264, but in the end we settled on using raw frames from our cameras
                    to minimize latency as much as possible, since at the 2023 RoboCup,
                    our camera's high latency has caused a lot of trouble.
				</p>
				<h2>Experiments and Testing</h2>
				<p>
					We have conducted image recognition tests, and tested the movement of
					our robot in our school. Our image recognition model successfully
					recognized hazmat labels with an 80% accuracy. We have also tested
                    our robotic arm controls, they worked good enough to gain us a decent amount
                    of points at the 2024 Hungarian competition (see the main page).
				</p>
				<h2>Software packages used</h2>
				<table>
					<tbody>
						<tr class="odd">
							<td>
								<a
									href="https://git.kernel.org/pub/scm/libs/libgpiod/libgpiod.git/about/"
									target="_blank"
								>
									libgpiod
								</a>
							</td>
							<td>DC motor control, sensor control, stepper control</td>
						</tr>
						<tr class="even">
							<td>
								<a href="https://opencv.org/" target="_blank"> OpenCV </a>
							</td>
							<td>Image recognition</td>
						</tr>
						<tr class="odd">
							<td>
								<a href="https://ffmpeg.org/" target="_blank"> FFmpeg </a>
							</td>
							<td>
								Video streaming, Image recognition backend (used by OpenCV)
							</td>
						</tr>
						<tr class="even">
							<td>
								<a href="https://www.libsdl.org/" target="_blank"> SDL2 </a>
							</td>
							<td>
								Used by KókányControl to process keyboard input and display
								video
							</td>
						</tr>
						<tr class="odd">
							<td>
								<a href="https://github.com/libsdl-org/SDL_ttf" target="_blank">
									SDL2_ttf
								</a>
							</td>
							<td>
								Used by KókányControl to draw text for displaying sensor data
							</td>
						</tr>
						<tr class="even">
							<td>
								<a href="https://github.com/libsdl-org/SDL_net" target="_blank">
									SDL2_net
								</a>
							</td>
							<td>Used by KókányControl to handle networking</td>
						</tr>
						<tr class="odd">
							<td>
								<a href="https://zbar.sourceforge.net/" target="_blank">
									libzbar
								</a>
							</td>
							<td>Used for QR code detection</td>
						</tr>
						<tr class="even">
							<td>
								<a
									href="https://github.com/ultralytics/ultralytics"
									target="_blank"
								>
									YoloV8 (small)
								</a>
							</td>
							<td>The model we use for object detection</td>
						</tr>
						<tr class="even">
							<td>
								<a
									href="https://universe.roboflow.com/new-workspace-xqnz7/rmrc-dqa9p"
									target="_blank"
								>
									RMRC Dataset
								</a>
							</td>
							<td>Dataset used for training our image recognition model</td>
						</tr>
						<tr class="odd">
							<td>
								<a href="https://undefined-medium.com/" target="_blank">
									undefined medium
								</a>
							</td>
							<td>The font used in kokanyctl</td>
						</tr>
						<tr class="even">
							<td>
								<a
									href="https://github.com/zsoltiv/libhwpwm/tree/master"
									target="_blank"
								>
									libhwpwm
								</a>
							</td>
                            <td>
                                A C library to interface with the
                                <a href="https://www.kernel.org/doc/html/latest/driver-api/pwm.html#using-pwms-with-the-sysfs-interface">
                                    Linux PWM userspace API
                                </a>
                            </td>
						</tr>
					</tbody>
				</table>
				<h2>What the team has learned so far</h2>
				<ul>
					<li>
						We gained experience with Fusion360, TinkerCAD and 3D printing
					</li>
					<li>
						<p>
							Using C enabled us to learn a lot about interacting with hardware
							at a lower level:
						</p>

						<ul>
							<li>Using standard Linux APIs</li>
							<li>Using I2C to communicate with integrated circuits</li>
							<li>Reading datasheets</li>
							<li>
								Learned to embed FFmpeg in C programs using their various
								<a
									href="https://ffmpeg.org/doxygen/trunk/index.html"
									target="_blank"
									>libraries</a>
							</li>
                            <li>Learned a lot about the quirks of PWM drivers in Linux, and their userspace API</li>
						</ul>
					</li>
				</ul>
			</div>
		</div>
	</body>
</html>
