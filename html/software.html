<!DOCTYPE html>
<html lang="en">
	<head>
		<meta charset="UTF-8" />
		<meta name="viewport" content="width=device-width, initial-scale=1.0" />
		<link rel="stylesheet" href="../css/style.css" />
		<title>Kókány Corporations</title>
	</head>
	<body>
		<div>
			<nav>
				<div class="logo">
					<div class="img">
						<img src="/img/kc-logov2.png" alt="logo" class="avatar log" />
					</div>
					<h1>Kókány Corporations</h1>
				</div>
				<ul>
					<li><a href="../index.html">Team</a></li>
					<li>
						<a href="software.html" class="active">Our Software</a>
					</li>
					<li>
						<a href="hardware.html">Our Hardware</a>
					</li>
					<li>
						<a href="attempts.html">Attempts</a>
					</li>
				</ul>
			</nav>
		</div>
		<div class="content">
			<div class="main-text">
				<h1>Our Software</h1>
				<h2>Communication</h2>
				<p>
					We interact with our robot using a few custom programs, named
					KókányControl (kokanyctl) and KókányRecognize (kokanyrecognize).
				</p>
				<p>
					KókányControl has a graphical interface for displaying the video and
					the sensor data it receives from Kókánybot. It takes keyboard input,
					and sends commands to the Raspberry Pi. It also recognizes QR codes
					that appear on Kókánybot’s camera.
				</p>
				<p>
					KókányRecognize was written to reduce the complexity of KókányControl,
					since image recognition functionality is only needed in a few runs,
					and we can just launch KókányRecognize whenever we need it. This also
					enabled us to build KókányControl in pure C, since we would have
					needed to use C++ to build the image recognition bits (which uses
					OpenCV).
				</p>
				<p>The robot can be controlled via WiFi (2.4Ghz) or Ethernet.</p>

				<h2>Training an object detection model</h2>

				<p>
					Last time, one of the most challanging aspects of the competition was
					the object detection feature robots needed. Back in in Bordeaux, we
					failed to detect anything and got 0 points for object detection. We
					definitely needed to improve on that.
				</p>

				<p>
					Firstly, we found a large enough dataset on the internet (1k+ photos),
					which was a godsend because it spared us from having to manually take
					pictures and tag them.
				</p>

				<p>
					Secondly, we switched from the outdated YoloV5 to the up-to-date
					YoloV8 which is supposedly better in every single way. We chose the
					small version, because the larger the model, the slower inference is,
					and while performance doesn’t really matter when training, one can
					simply leave their computer running while they are not home, it is
					still vital during inference, because our laptops are not on par with
					our workstations at home.
				</p>

				<p>
					To use our model, we wrote a script named kokanyrecognize at the last
					minute. It wasn’t very performant nor really clean, so we spent a
					significant amount of time working on it. So far it was rewritten to
					use the new model, however it still has a long way to go.
				</p>
				<p>Here is the result so far:</p>
				<img
					src="../img/inference.jpg"
					alt="Image showcasing the accuracy of our model"
				/>
				<ul>
					<li>
						Newer formats use more CPU cycles, increasing temperature and power
						draw
					</li>
					<li>We use a Cat5e ethernet cable so the savings in size are moot</li>
					<li>The Pi 4 has a hardware H.264 encoder</li>
				</ul>
				<p>
					For audio, we considered several formats, both lossy and lossless, but
					since audio data isn’t that large anyway, and compressing it still
					takes some processing power, we settled on using plain WAVE.
				</p>
				<p>
					To implement the actual capturing and transmission we used a simple
					FFmpeg commandline in a
					<a
						href="https://github.com/zsoltiv/kokanybot/blob/master/kokanystream.sh"
						target="_blank"
						>shell script</a
					>. We picked FFmpeg because as its
					<a href="https://ffmpeg.org" target="_blank">project page</a>
					suggests, it's a multimedia swiss army knife that can handle a wide
					array of use cases, including
					<a
						href="http://trac.ffmpeg.org/wiki/StreamingGuide#Pointtopointstreaming"
						target="_blank"
						>point-to-point</a
					>
					streaming via TCP, allowing us to reduce the infrastructure required
					to transmit the audiovisual data.
				</p>
				<p>
					On the client side, we implemented the decoding of the video data
					using FFmpeg’s libavformat and libavcodec libraries. Rendering the
					video frames was tricky to figure out because pretty much all video
					encoders store pixels in YCbCr colour space, which SDL isn’t the best
					for.
				</p>
				<p>
					The APIs of the libav* libraries are <b>huge</b>. Thankfully we only
					really needed the
					<a
						href="https://ffmpeg.org/doxygen/trunk/group__lavc__encdec.html"
						target="_blank"
						>high level decoding API</a
					>. The
					<a
						href="https://github.com/namndev/FFmpegTutorial/blob/master/learn-ffmpeg-libav-the-hard-way.md"
						target="_blank"
						>Learn FFmpeg libav the Hard Way</a
					>
					tutorial combined with the
					<a
						href="https://git.ffmpeg.org/gitweb/ffmpeg.git/tree/HEAD:/doc/examples"
						target="_blank"
						>examples</a
					>
					in the project's documentation also made things much easier.
				</p>
				<h2>Simplifying our software with device trees</h2>
				<p>
					last year we used a library called
					<a
						href="https://git.kernel.org/pub/scm/utils/i2c-tools/i2c-tools.git/about/"
					>
						libi2c</a
					>
					to control some of our stepper motors using an external
					<a href="https://www.microchip.com/en-us/product/MCP23017"
						>MCP23017 GPIO expander</a
					>. We wanted to make our software more flexible, and the best way to
					do this was ripping out the useless code and instead using the right
					tools for the job.
				</p>
				<p>
					While programs are allowed to just use libi2c, it adds (often
					redundant) extra code to a project. The Kernel actually has drivers
					for a lot of common ICs such as GPIO expanders, like the MCP23017 we
					use.
				</p>
				<p>
					One improvement we've made was deleting the I2C parts from our code,
					and instead using the
					<a
						href="https://elixir.bootlin.com/linux/latest/source/drivers/pinctrl/pinctrl-mcp23s08.c"
						>kernel driver</a
					>
					instead. This is great because we no longer need separate code to use
					the GPIO expanders, since they're just <code>/dev/gpiochip</code>s
					now.
				</p>
				<p>
					To achieve this, we wrote a
					<a
						href="https://github.com/zsoltiv/kokanybot/blob/master/mcp23017.dts"
						>device tree overlay</a
					>. Device tree overlays are kind of like patch files for
					<a
						href="https://www.kernel.org/doc/html/latest/devicetree/usage-model.html"
						>device trees</a
					>. This file allows us to tell Linux what chips are available on which
					I2C addresses.
				</p>
				<div class="paragraphs">
					<h2>Setup and packup of the robot and operator station</h2>
					<p>
						To set up the robot, we charge the 18650 batteries, put them into
						the battery pack, and flip a switch on our robot.
					</p>
					<p>
						For the operating station, we use a laptop. We connect to our
						portable WiFi router, and access the onboard computer’s web
						interface.
					</p>
				</div>
				<div class="paragraphs">
					<h2>Mission strategy</h2>
					<p>
						We use standard, easily acquirable parts, which makes our robot
						truly rapidly manufacturable.
					</p>
				</div>
				<h2>Video and audio streaming</h2>
				<p>
					During tests, operators are only allowed to see the arenas from their
					robot’s point of view. This meant we needed a way to find a way to
					display the video data from the robot’s camera.
				</p>
				<p>
					Multimedia related tasks are surprisingly computation heavy when one
					is working with embedded systems. The CPU in the Raspberry Pi 4B+ is
					fairly capable, however we also had to consider power draw and thermal
					related problems. We considered several video formats: H.265, AV1 and
					H.264, but in the end we settled on using good old H.264 for a few
					reasons:
				</p>
				<div class="paragraphs">
					<h2>Experiments and Testing</h2>
					<p>
						We have conducted image recognition tests, and tested the movement
						of our robot in our school. Our image recognition model successfully
						recognized hazmat labels with an 80% accuracy.
					</p>
				</div>
				<h2>Software packages used</h2>
				<table>
					<tbody>
						<tr class="odd">
							<td>
								<a
									href="https://git.kernel.org/pub/scm/libs/libgpiod/libgpiod.git/about/"
									target="_blank"
								>
									libgpiod
								</a>
							</td>
							<td>DC motor control, sensor control, stepper control</td>
						</tr>
						<tr class="even">
							<td>
								<a href="https://opencv.org/" target="_blank"> OpenCV </a>
							</td>
							<td>Image recognition</td>
						</tr>
						<tr class="odd">
							<td>
								<a href="https://ffmpeg.org/" target="_blank"> FFmpeg </a>
							</td>
							<td>
								Video streaming, Image recognition backend (used by OpenCV)
							</td>
						</tr>
						<tr class="even">
							<td>
								<a href="https://www.libsdl.org/" target="_blank"> SDL2 </a>
							</td>
							<td>
								Used by KókányControl to process keyboard input and display
								video
							</td>
						</tr>
						<tr class="odd">
							<td>
								<a href="https://github.com/libsdl-org/SDL_ttf" target="_blank">
									SDL2_ttf
								</a>
							</td>
							<td>
								Used by KókányControl to draw text for displaying sensor data
							</td>
						</tr>
						<tr class="even">
							<td>
								<a href="https://github.com/libsdl-org/SDL_net" target="_blank">
									SDL2_net
								</a>
							</td>
							<td>Used by KókányControl to handle networking</td>
						</tr>
						<tr class="odd">
							<td>
								<a href="https://zbar.sourceforge.net/" target="_blank">
									libzbar
								</a>
							</td>
							<td>Used for QR code detection</td>
						</tr>
						<tr class="even">
							<td>
								<a
									href="https://github.com/ultralytics/ultralytics"
									target="_blank"
								>
									YoloV8 (small)
								</a>
							</td>
							<td>The model we use for object detection</td>
						</tr>
						<tr class="even">
							<td>
								<a
									href="https://universe.roboflow.com/new-workspace-xqnz7/rmrc-dqa9p"
									target="_blank"
								>
									RMRC Dataset
								</a>
							</td>
							<td>Dataset used for training our image recognition model</td>
						</tr>
						<tr class="odd">
							<td>
								<a href="https://undefined-medium.com/" target="_blank">
									undefined medium
								</a>
							</td>
							<td>The font used in kokanyctl</td>
						</tr>
					</tbody>
				</table>
				<div class="paragraphs">
					<h2>What the team has learned so far</h2>
					<ul>
						<li>
							We gained experience with Fusion360, TinkerCAD and 3D printing
						</li>
						<li>
							<p>
								Using C enabled us to learn a lot about interacting with
								hardware at a lower level:
							</p>

							<ul>
								<li>Using standard Linux APIs</li>
								<li>Using I2C to communicate with integrated circuits</li>
								<li>Reading datasheets</li>
								<li>
									Learned to embed FFmpeg in C programs using their various
									<a
										href="https://ffmpeg.org/doxygen/trunk/index.html"
										target="_blank"
										>libraries</a
									>
								</li>
							</ul>
						</li>
					</ul>
				</div>
			</div>
		</div>
	</body>
</html>
