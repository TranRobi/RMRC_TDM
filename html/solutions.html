<!DOCTYPE html>
<html lang="en">
	<head>
		<meta charset="UTF-8" />
		<meta name="viewport" content="width=device-width, initial-scale=1.0" />
		<link rel="stylesheet" href="../css/style.css" />
		<title>Kókány Corporations</title>
	</head>
	<body>
		<div>
			<nav>
				<div class="logo">
					<div class="img">
						<img src="../img/pfp3.jpg" alt="logo" class="log" />
					</div>
					<h1>Kókány Corporations</h1>
				</div>
				<ul>
					<li><a href="../index.html">Team</a></li>
					<li>
						<a href="solutions.html" class="active">Solutions</a>
					</li>
					<li>
						<a href="software.html">Our Software</a>
					</li>
					<li>
						<a href="hardware.html">Our Hardware</a>
					</li>
				</ul>
			</nav>
		</div>
		<div class="content">
			<div class="system">
				<div class="main-text">
					<h1>Problems we encountered and our solutions</h1>
				<h2>Training an object detection model</h2>

				<p>
					Last time, one of the most challanging aspects of the competition was
					the object detection feature robots needed. Back in in Bordeaux, we
					failed to detect anything and got 0 points for object detection. We
					definitely needed to improve on that.
				</p>

				<p>
					Firstly, we found a large enough dataset on the internet (1k+ photos),
					which was a godsend because it spared us from having to manually take
					pictures and tag them.
				</p>

				<p>
					Secondly, we switched from the outdated YoloV5 to the up-to-date
					YoloV8 which is supposedly better in every single way. We chose the
					small version, because the larger the model, the slower inference is,
					and while performance doesn’t really matter when training, one can
					simply leave their computer running while they are not home, it is still
					vital during inference, because our laptops are not on par with our
					workstations at home.
				</p>

				<p>
					To use our model, we wrote a script named kokanyrecognize at the last
					minute. It wasn’t very performant nor really clean, so we spent a
					significant amount of time working on it. So far it was rewritten
					to use the new model, however it still has a long way to go.
				</p>

				<p>Here is the result so far:</p>
				<img
					src="../img/inference.jpg"
					alt="Image showcasing the accuracy of our model"
				/>
				<h2>Video and audio streaming</h2>
				<p>
					During tests, operators are only allowed to see the arenas from their
					robot’s point of view. This meant we needed a way to find a way to
					display the video data from the robot’s camera.
				</p>
				<p>
					Multimedia related tasks are surprisingly computation heavy when one
					is working with embedded systems. The CPU in the Raspberry Pi 4B+ is
					fairly capable, however we also had to consider power draw and thermal
					related problems. We considered several video formats: H.265, AV1 and
					H.264, but in the end we settled on using good old H.264 for a few
					reasons:
				</p>
				<ul>
					<li>
						Newer formats use more CPU cycles, increasing temperature and power
						draw
					</li>
					<li>We use a Cat5e ethernet cable so the savings in size are moot</li>
					<li>The Pi 4 has a hardware H.264 encoder</li>
				</ul>
				<p>
					For audio, we considered several formats, both lossy and lossless, but
					since audio data isn’t that large anyway, and compressing it still
					takes some processing power, we settled on using plain WAVE.
				</p>
				<p>
					To implement the actual capturing and transmission we used a simple
					FFmpeg commandline in a
					<a
						href="https://github.com/zsoltiv/kokanybot/blob/master/kokanystream.sh"
						target="_blank"
						>shell script</a
					>. We picked FFmpeg because as its
					<a href="https://ffmpeg.org" target="_blank">project page</a>
					suggests, it's a multimedia swiss army knife that can handle a wide
					array of use cases, including
					<a
						href="http://trac.ffmpeg.org/wiki/StreamingGuide#Pointtopointstreaming"
						target="_blank"
						>point-to-point</a
					>
					streaming via TCP, allowing us to reduce the infrastructure required
					to transmit the audiovisual data.
				</p>
				<p>
					On the client side, we implemented the decoding of the video data
					using FFmpeg’s libavformat and libavcodec libraries. Rendering the
					video frames was tricky to figure out because pretty much all video
					encoders store pixels in YCbCr colour space, which SDL isn’t the best
					for.
				</p>
				<p>
					The APIs of the libav* libraries are <b>huge</b>. Thankfully we only
					really needed the
					<a
						href="https://ffmpeg.org/doxygen/trunk/group__lavc__encdec.html"
						target="_blank"
						>high level decoding API</a
					>. The
					<a
						href="https://github.com/namndev/FFmpegTutorial/blob/master/learn-ffmpeg-libav-the-hard-way.md"
						target="_blank"
						>Learn FFmpeg libav the Hard Way</a
					>
					tutorial combined with the
					<a
						href="https://git.ffmpeg.org/gitweb/ffmpeg.git/tree/HEAD:/doc/examples"
						target="_blank"
						>examples</a
					>
					in the project's documentation also made things much easier.
				</p>
					<div class="paragraphs">
						<h2>Setup and packup of the robot and operator station</h2>
						<p>
							To set up the robot, we charge the 18650 batteries, put them into
							the battery pack, and flip a switch on our robot.
						</p>
						<p>
							For the operating station, we use a laptop. We connect to our
							portable WiFi router, and access the onboard computer’s web
							interface.
						</p>
					</div>
					<div class="paragraphs">
						<h2>Mission strategy</h2>
						<p>
							We use standard, easily acquirable parts, which makes our robot
							truly rapidly manufacturable.
						</p>
						<a href="hardware.html" class="robot">-About Hardware*</a>
					</div>
					<div class="paragraphs">
						<h2>Experiments and Testing</h2>
						<p>
							We have conducted image recognition tests, and tested the movement
							of our robot in our school. Our image recognition model
							successfully recognized hazmat labels with an 80% accuracy.
						</p>
						<a href="software.html" class="robot">*About Software*</a>
					</div>
					<div class="paragraphs">
						<h2>Strengths and real-life applications</h2>
						<p>
							We focused on making our robot compact, easily manufacturable and
							well-rounded, with portable software so the same program can be
							ported to different single board computers.
						</p>
					</div>
					<div class="paragraphs">
						<h2>What the team has learned so far</h2>
						<ul>
							<li>
								We gained experience with Fusion360, TinkerCAD and 3D
								printing
							</li>
							<li>
								<p>
									Using C enabled us to learn a lot about interacting with
									hardware at a lower level:
								</p>

								<ul>
									<li>Using standard Linux APIs</li>
									<li>Using I2C to communicate with integrated circuits</li>
									<li>Reading datasheets</li>
									<li>
										Learned to embed FFmpeg in C programs using their various
										<a
											href="https://ffmpeg.org/doxygen/trunk/index.html"
											target="_blank"
											>libraries</a
										>
									</li>
								</ul>
							</li>
						</ul>
					</div>
				</div>
			</div>
		</div>
	</body>
</html>
